#!/usr/bin/env bash

set -o errexit
set -o nounset

usage(){
  cat <<EOF
Use pg_dump to create a backup of a postres database. Then copy it to an S3
bucket.

Usage: db-backup [flags] [<database>]

Parameters:
  <database>              name of the database.  

Flags:
  -h                      display this help
  -n                      dry run: do not perform geocoding lookup- just print url


Environment:
  BACKUP_DIR   Local directory to write backups
  NUM_BACKUPS  Number of backups to retain locally. Default: 10
  S3_BUCKET    Name of the S3 bucket: if unset, an S3 copy will not occur
  S3_PATH      S3 path to write backups. The path requires a trailing slash. Default: /.
  PGDATABASE
  PGHOST
  PGPORT
  PGUSER
  PGPASSWORD
  AWS*

Description: 

This script is meant to be run via cron on a periodic (daily) basis.  It uses
pg_dump to create a backup of a postgres database.  

Each backup will have a name like <database>-<date>:
  database - PGDATABASE
  date     - date --iso-8601=seconds (2023-02-25T22:42:16-05:00)

A symlink, latest, points to the latest backup.

NUM_BACKUPS (default 10) will be retained.

EOF
}


# set defaults for vars that are required
PGDATABASE=${PGDATABASE:=""}
BACKUP_DIR=${BACKUP_DIR:=/var/tmp/backups}
S3_BUCKET=${S3_BUCKET:=""}
S3_PATH=${S3_PATH:=""}
# needs to be exported for perl subshell
export NUM_BACKUPS=${NUM_BACKUPS:=10} 
DRYRUN=0          

fail() {
  echo $1 >&2
  exit 1
}

init() {

  OPTIND=1

  while getopts "hn?" opt; do
      case "$opt" in
      h|\?)
        usage
        exit 0
        ;;
      n)
        DRYRUN=1
        ;;
      esac
  done

  shift $((OPTIND-1))

  if [[ $# -gt 0 ]]; then
    PGDATABASE=$1
  fi

  if [[ -z ${PGDATABASE} ]]; then
    fail "A database was not specified via environment or as a parameter"
  fi

  
}

main() {
  # run the init code
  init $*

  # the guts of the script
 
  # create the backup directory if required
  if [[ ! -d ${BACKUP_DIR} ]]; then
    if [[ $DRYRUN -eq 0 ]]; then 
      mkdir ${BACKUP_DIR} || fail "unable to create directory ${BACKUP_DIR}"
    fi
  elif [[ ! -w ${BACKUP_DIR} ]]; then
    fail "${BACKUP_DIR} is not writable"
  fi
 
  backup_file="${PGDATABASE}-$(date --iso-8601=seconds)"

  if [[ ${DRYRUN} -eq 1 ]]; then
    echo "pg_dump -d \"sslmode=require\" -Fc -f ${BACKUP_DIR}/${backup_file}"
    echo "ln -sf ${backup_file} ${BACKUP_DIR}/${PGDATABASE}-latest"
  else
    pg_dump -d "sslmode=require" -Fc -f "${BACKUP_DIR}/${backup_file}"
    ln -sf "${backup_file}" "${BACKUP_DIR}/${PGDATABASE}-latest"
  fi

  # remove backup files in excess of the limit
  files_to_rm=$(find ${BACKUP_DIR} -name "${PGDATABASE}-*" -print | sort | perl -nE 'print if $. >$ENV{q/NUM_BACKUPS/}')

  if [[ ${DRYRUN} -eq 1 ]]; then
    echo "rm -f ${files_to_rm}"
  else
    rm -f "${files_to_rm}"
  fi

  if [[ -z ${S3_BUCKET} ]]; then
    exit
  fi

  # copy files to S3
  if [[ ${DRYRUN} -eq 1 ]]; then
    echo "aws s3 sync ${BACKUP_DIR}/ backups:${S3_BUCKET}/${S3_PATH}"
  else
    aws s3 sync --follow-symlinks "${BACKUP_DIR}/" "s3://${S3_BUCKET}/${S3_PATH}"
  fi

}

main $*
